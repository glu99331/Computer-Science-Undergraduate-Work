dfA
### confusion matrix for the linear relationship
dfA <- count(post_pred_summary_03_line_c,  vars=c("y","pred_y"))
post_pred_summary_03_line_c %>%  count(y)
### confusion matrix for the linear relationship
post_pred_summary_03_line_c %>%  count(y) %>%  count(pred_y)
post_pred_summary_03_line_c
post_pred_summary_03_line_c %>%  count(y) %>%  count(pred_y)
post_pred_summary_03_line_c
post_pred_summary_03_line_c %>%  count(pred_y)
### confusion matrix for the linear relationship
post_pred_summary_03_line_c
post_pred_summary_03_line_c %>%  count(pred_y) %>%  count(y)
post_pred_summary_03_line_c
post_pred_summary_03_line_c %>%  count(pred_y, y) %>%
### confusion matrix for the linear relationship
post_pred_summary_03_line_c
post_pred_summary_03_line_c %>%  count(pred_y, y)
### confusion matrix for the linear relationship
post_pred_summary_03_line_c
post_pred_summary_03_line_c %>%  count(pred_y, y)
### confusion matrix for the linear relationship
post_pred_summary_03_line_c %>%  count(pred_y, y)
### confusion matrix for the linear relationship
post_pred_summary_03_line_c %>%  count(y, pred_y)
### confusion matrix for the linear relationship
##post_pred_summary_03_line_c %>%  count(y, pred_y)
count_(post_pred_summary_03_line_c, vars=c("pred_y", "y"))
### confusion matrix for the linear relationship
post_pred_summary_03_line_c %>%  count(y, pred_y)
### confusion matrix for the linear relationship
### confusion matrix for the cubic relationship
### confusion matrix for the linear relationship
post_pred_summary_03_cube_c %>% count(y, pred_y)
### confusion matrix for the linear relationship
post_pred_summary_03_line_c %>% count(y, pred_y)
count_(post_pred_summary_03_line_c, vars=c("pred_y", "y"))
### confusion matrix for the linear relationship
post_pred_summary_03_line_c %>% count(pred_y, y)
count_(post_pred_summary_03_line_c, vars=c("pred_y", "y"))
### confusion matrix for the linear relationship
post_pred_summary_03_line_c %>% count(pred_y, y)
dfaB <- count_(post_pred_summary_03_line_c, vars=c("pred_y", "y"))
dfaB
post_pred_summary_03_line_c <- post_pred_summary_03_line_b %>%
left_join(train_03 %>% tibble::rowid_to_column("pred_id"),
by = "pred_id")
post_pred_summary_03_cube_c <- post_pred_summary_03_cube_b %>%
left_join(train_03 %>% tibble::rowid_to_column("pred_id"),
by = "pred_id")
post_pred_summary_03_line_b <- post_pred_summary_03_line %>% mutate(pred_y = ifelse(mu_avg > 0.5, 1, 0))
post_pred_summary_03_cube_b <- post_pred_summary_03_cube %>% mutate(pred_y = ifelse(mu_avg >0.5, 1, 0))
post_pred_summary_03_line %>%
mutate(type = "linear relationship") %>%
bind_rows(post_pred_summary_03_cube %>%
mutate(type = "cubic relationship")) %>%
left_join(train_03 %>% tibble::rowid_to_column("pred_id"),
by = "pred_id") %>%
ggplot(mapping = aes(x = x)) +
geom_ribbon(mapping = aes(ymin = mu_q05,
ymax = mu_q95,
group = type),
fill = "steelblue", alpha = 0.5) +
geom_line(mapping = aes(y = mu_avg,
group = type),
color = "navyblue", size = 1.15) +
geom_point(mapping = aes(y = y),
size = 2.5, alpha = 0.25) +
facet_grid( . ~ type) +
labs(y = "y or probability") +
theme_bw()
dim(post_pred_summary_03_line)
dim(post_pred_summary_03_cube)
summarize_glm_pred_from_laplace <- function(mvn_result, Xtest, num_samples)
{
# generate posterior samples of the beta parameters
betas <- generate_glm_post_samples(mvn_result, num_samples)
# data type conversion
betas <- as.matrix(betas)
# make posterior predictions on the test set
pred_test <- post_glm_pred_samples(Xtest, betas)
# calculate summary statistics on the posterior predicted probability
# summarize over the posterior samples
pred_avg = mean(pred_test$eta_mat)
# posterior mean, should you summarize along rows (rowMeans) or
# summarize down columns (colMeans) ???
mu_avg = rowMeans(pred_test$mu_mat)
# posterior quantiles
mu_q05 = apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.05)
mu_q95 = apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.95)
# book keeping
tibble::tibble(
mu_avg = mu_avg,
mu_q05 = mu_q05,
mu_q95 = mu_q95
) %>%
tibble::rowid_to_column("pred_id")
}
post_pred_summary_03_line <- summarize_glm_pred_from_laplace(laplace_03_line, Xmat_03_line, 2500)
post_pred_summary_03_cube <- summarize_glm_pred_from_laplace(laplace_03_cube, Xmat_03_cube, 2500)
dim(post_pred_summary_03_line)
dim(post_pred_summary_03_cube)
post_pred_summary_03_line %>%
mutate(type = "linear relationship") %>%
bind_rows(post_pred_summary_03_cube %>%
mutate(type = "cubic relationship")) %>%
left_join(train_03 %>% tibble::rowid_to_column("pred_id"),
by = "pred_id") %>%
ggplot(mapping = aes(x = x)) +
geom_ribbon(mapping = aes(ymin = mu_q05,
ymax = mu_q95,
group = type),
fill = "steelblue", alpha = 0.5) +
geom_line(mapping = aes(y = mu_avg,
group = type),
color = "navyblue", size = 1.15) +
geom_point(mapping = aes(y = y),
size = 2.5, alpha = 0.25) +
facet_grid( . ~ type) +
labs(y = "y or probability") +
theme_bw()
post_glm_pred_samples <- function(Xnew, Bmat)
{
# calculate the linear predictor at all prediction points and posterior samples
eta_mat <- as.matrix(Xnew) %*% t(as.matrix(Bmat))
# calculate the event probability
mu_mat <- boot::inv.logit(eta_mat)
# book keeping
list(eta_mat = eta_mat, mu_mat = mu_mat)
}
generate_glm_post_samples <- function(mvn_result, num_samples)
{
# specify the number of unknown beta parameters
length_beta <- ncol(mvn_result$var_matrix)
# generate the random samples
beta_samples <- MASS::mvrnorm(n = num_samples,
mu = mvn_result$mode,
Sigma = mvn_result$var_matrix)
# change the data type and name
beta_samples %>%
as.data.frame() %>% tbl_df() %>%
purrr::set_names(sprintf("beta_%02d", (1:length_beta) - 1))
}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
train_01 <- readr::read_csv("https://raw.githubusercontent.com/jyurko/CS_1675_Spring_2020/master/hw_data/hw04/train_01.csv", col_names = TRUE)
train_01 %>% glimpse()
train_01 %>%  ggplot(mapping = aes(x = x, y = y )) +
geom_point() + geom_smooth(method = 'lm')
### your code here
Xmat_01 <- model.matrix(y ~ x, data = train_01)
info_01 <- list(
yobs = train_01$y,
design_matrix = Xmat_01,
mu_beta = 0,
tau_beta = 5,
sigma_rate = 1
)
lm_logpost <- function(unknowns, my_info)
{
# specify the number of unknown beta parameters
length_beta <- ncol(as.matrix(my_info$design_matrix))
# extract the beta parameters from the `unknowns` vector
beta_v <- unknowns[1:length_beta]
# extract the unbounded noise parameter, varphi
lik_varphi <- unknowns[length_beta+1]
# back-transform from varphi to sigma
lik_sigma <- exp(lik_varphi)
# extract design matrix
X <- my_info$design_matrix
# calculate the linear predictor
mu <- as.vector(X %*% as.matrix(beta_v))
# evaluate the log-likelihood
log_lik <- sum(dnorm(x=my_info$yobs, mean=mu, sd=lik_sigma,log=TRUE))
# evaluate the log-prior
log_prior_beta <- sum(dnorm(x=beta_v, mean=my_info$mu_beta,sd=my_info$tau_beta,log=TRUE))
log_prior_sigma <- log(my_info$sigma_rate) - (my_info$sigma_rate * lik_sigma)
# add the mean trend prior and noise prior together
log_prior <- log_prior_beta + log_prior_sigma
# account for the transformation
log_derive_adjust <- lik_varphi
# sum together
log_lik + log_prior + log_derive_adjust
}
my_laplace <- function(start_guess, logpost_func, ...)
{
# code adapted from the `LearnBayes`` function `laplace()`
fit <- optim(start_guess,
logpost_func,
gr = NULL,
...,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
mode <- fit$par
h <- -solve(fit$hessian)
p <- length(mode)
int <- p/2 * log(2*pi) + 0.5*log(det(h)) + logpost_func(mode,...)
list(mode = mode,
var_matrix = h,
log_evidence = int,
converge = ifelse(fit$convergence == 0,
"YES",
"NO"),
iter_counts = fit$counts[1])
}
laplace_01 <- my_laplace(rep(0,ncol(Xmat_01)+1), lm_logpost, info_01)
laplace_01$mode
### your code here
laplace_01$var_matrix
generate_lm_post_samples <- function(mvn_result, length_beta, num_samples)
{
MASS::mvrnorm(n = num_samples,
mu = mvn_result$mode,
Sigma = mvn_result$var_matrix) %>%
as.data.frame() %>% tbl_df() %>%
purrr::set_names(c(sprintf("beta_%02d", (1:length_beta) - 1), "varphi")) %>%
mutate(sigma = exp(varphi))
}
set.seed(87123)
post_samples_01 <- generate_lm_post_samples(laplace_01, ncol(Xmat_01), 2500)
post_samples_01 %>% summary()
post_samples_01 %>% ggplot()+
geom_histogram(mapping=(aes(x = beta_01)), bins=55)
###
prob <- 1-pnorm(0, mean(post_samples_01$beta_01), sd(post_samples_01$beta_01))
prob
post_lm_pred_samples <- function(Xnew, Bmat, sigma_vector)
{
# number of new prediction locations
M <- nrow(Xnew)
# number of posterior samples
S <- nrow(Bmat)
# matrix of linear predictors
Umat <- as.matrix(Xnew %*% t(as.matrix(Bmat)))
# assmeble matrix of sigma samples
Rmat <- matrix(rep(sigma_vector, M), M, byrow = TRUE)
# generate standard normal and assemble into matrix
Zmat <- matrix(rnorm(M*S), M, byrow = TRUE)
# calculate the random observation predictions
Ymat <- Umat + Rmat*Zmat
# package together
list(Umat = Umat, Ymat = Ymat)
}
make_post_lm_pred <- function(Xnew, post)
{
Bmat <- post %>% select(starts_with("beta_")) %>% as.matrix()
sigma_vector <- post %>% pull(sigma)
post_lm_pred_samples(Xnew, Bmat, sigma_vector)
}
post_pred_samples_01 <- make_post_lm_pred(Xmat_01, post_samples_01)
dim(post_pred_samples_01$Umat)
dim(post_pred_samples_01$Ymat)
### error of the first posterior sample
error_01_post_01 <- post_pred_samples_01$Umat[,1] - train_01$y
### error of the second posterior sample
error_01_post_02 <- post_pred_samples_01$Umat[,2] - train_01$y
### error of the third posterior sample
error_01_post_03 <- post_pred_samples_01$Umat[,3] - train_01$y
length(error_01_post_01)
### a 2 x 4 matrix
matrix(1:8, nrow = 2, byrow = TRUE)
### a vector length 2
c(1, 2)
### subtracting the two yields a matrix
matrix(1:8, nrow = 2, byrow = TRUE) - c(1, 2)
absE01mat <- abs(post_pred_samples_01$Umat-train_01$y)
### dimensions?
dim(absE01mat)
MAE_01 <- colMeans(absE01mat)
class(MAE_01)
### length?
length(MAE_01)
MAE_01[1] == mae_01_post_01
MAE_01[2] == mae_01_post_02
MAE_01[3] == mae_01_post_03
MAE_01 %>% quantile()
train_02 <- readr::read_csv("https://raw.githubusercontent.com/jyurko/CS_1675_Spring_2020/master/hw_data/hw04/train_02.csv", col_names = TRUE)
test_02 <- readr::read_csv("https://raw.githubusercontent.com/jyurko/CS_1675_Spring_2020/master/hw_data/hw04/test_02.csv", col_names = TRUE)
train_02 %>% ggplot() +
geom_point(mapping=aes(x=x, y=y, color='Train')) +
test_02 %>%
geom_point(mapping=aes(x=x, y=y, color='Test'))
make_spline_basis_mats <- function(J, train_data, test_data)
{
train_basis <- splines::ns(train_data$x, df=J)
knots_use_basis <- as.vector(attributes(train_basis)$knots)
train_matrix <- model.matrix(y~splines::ns(x, knots=knots_use_basis), data=train_data)
test_matrix <- model.matrix(y~splines::ns(x, knots = knots_use_basis), data=test_data)
tibble::tibble(
design_matrix = list(train_matrix),
test_matrix = list(test_matrix)
)
}
spline_matrices <- purrr::map_dfr(1:25, make_spline_basis_mats,
train_data = train_02,
test_data = test_02)
glimpse(spline_matrices)
spline_matrices
dim(spline_matrices$design_matrix[[1]])
dim(spline_matrices$test_matrix[[1]])
dim(spline_matrices$design_matrix[[2]])
dim(spline_matrices$test_matrix[[2]])
info_02_train <- list(
yobs = train_02$y,
mu_beta = 0,
tau_beta = 20,
sigma_rate = 1
)
manage_spline_fit <- function(Xtrain, logpost_func, my_settings)
{
my_settings$design_matrix <- Xtrain
init_beta <- rnorm(ncol(Xtrain))
init_varphi <- log(rexp(1, rate=my_settings$sigma_rate))
my_laplace(c(init_beta, init_varphi), logpost_func, my_info=my_settings)
}
set.seed(724412)
all_spline_models <- purrr::map(spline_matrices$design_matrix,
manage_spline_fit,
logpost_func = lm_logpost,
my_settings = info_02_train)
all_spline_models[[2]]
purrr::map_chr(all_spline_models, "converge")
calc_mae_from_laplace <- function(mvn_result, Xtrain, Xtest, y_train, y_test, num_samples)
{
# generate posterior samples from the approximate MVN posterior
post <- generate_lm_post_samples(mvn_result, ncol(Xtrain), num_samples)
# make posterior predictions on the training set
pred_train <- make_post_lm_pred(Xtrain, post)
# make posterior predictions on the test set
pred_test <- make_post_lm_pred(Xtest, post)
# calculate the error between the training set predictions
# and the training set observations
error_train <- as.matrix(pred_train$Umat)-y_train
# calculate the error between the test set predictions
# and the test set observations
error_test <- as.matrix(pred_test$Umat)-y_test
# calculate the MAE on the training set
mae_train <- colMeans(abs(error_train))
# calculate the MAE on the test set
mae_test <- colMeans(abs(error_test))
# book keeping, package together the results
mae_train_df <- tibble::tibble(
mae = mae_train
) %>%
mutate(dataset = "training") %>%
tibble::rowid_to_column("post_id")
mae_test_df <- tibble::tibble(
mae = mae_test
) %>%
mutate(dataset = "test") %>%
tibble::rowid_to_column("post_id")
# you must specify the order, J, associated with the spline model
mae_train_df %>%
bind_rows(mae_test_df) %>%
mutate(J = ncol(Xtrain))
}
set.seed(52133)
all_spline_mae_results <- purrr::pmap_dfr(list(all_spline_models,
spline_matrices$design_matrix,
spline_matrices$test_matrix),
calc_mae_from_laplace,
y_train = train_02$y,
y_test = test_02$y,
num_samples = 2500)
all_spline_mae_results %>%
ggplot(mapping = aes(x = as.factor(J), y=mae))+
geom_boxplot(mapping=aes(fill=dataset),
scale = scale_fill_brewer(palette = 'Set1'))
all_spline_mae_results %>% filter(J>3)%>%
ggplot(mapping=aes(x=J, y=mae))+
stat_summary(geom = 'line', fun.y = 'median')+
facet_wrap('dataset')
spline_evidence <- purrr::map_dbl(all_spline_models, "log_evidence")
spline_weights <- exp(spline_evidence)/sum(exp(spline_evidence))
tibble::tibble(
w = spline_weights
) %>%
tibble::rowid_to_column("J") %>%
ggplot(mapping = aes(x=as.factor(J), y=w))+
geom_bar(stat="identity")
train_03 <- readr::read_csv("https://raw.githubusercontent.com/jyurko/CS_1675_Spring_2020/master/hw_data/hw04/train_03.csv", col_names = TRUE)
train_03 %>% glimpse()
train_03 %>% count(y)
Xmat_03_line <- model.matrix(y~x, train_03)
Xmat_03_cube <- model.matrix(y~x+I(x^2)+I(x^3), train_03)
info_03_line <- list(
yobs = train_03$y,
design_matrix = Xmat_03_line,
mu_beta = 0,
tau_beta = 5
)
info_03_cube <- list(
yobs = train_03$y,
design_matrix = Xmat_03_cube,
mu_beta = 0,
tau_beta = 5
)
glm_logpost <- function(unknowns, my_info)
{
# extract the design matrix and assign to X
X <- my_info$design_matrix
# calculate the linear predictor
eta <- as.matrix(X) %*% as.matrix(unknowns)
# calculate the event probability
mu <- exp(eta)/(1+exp(eta))
# evaluate the log-likelihood
log_lik <- sum(my_info$yobs*log(mu) + (1-my_info$yobs)*log(1-mu))
# evaluate the log-prior
log_prior <- sum(dnorm(unknowns, my_info$mu_beta, my_info$tau_beta, log=TRUE))
# sum together
log_lik+log_prior
}
glm_logpost(c(-1,-1), info_03_line)
laplace_03_line <- my_laplace(rep(0,ncol(Xmat_03_line)),glm_logpost,info_03_line)
laplace_03_cube <- my_laplace(rep(0,ncol(Xmat_03_cube)),glm_logpost, info_03_cube)
sprintf("Line Model: [%.4f, %.4f]", qnorm(0.025,mean=laplace_03_line$mode[1], sd=5),
qnorm(0.975,mean=laplace_03_line$mode[1], sd = 5))
sprintf("Cube Model: [%.4f, %.4f]", qnorm(0.025,mean=laplace_03_cube$mode[1], sd=5),
qnorm(0.975,mean=laplace_03_cube$mode[1], sd = 5))
exp(laplace_03_line$log_evidence)/exp(laplace_03_cube$log_evidence)
cov2cor(solve(t(Xmat_03_cube) %*% Xmat_03_cube))
generate_glm_post_samples <- function(mvn_result, num_samples)
{
# specify the number of unknown beta parameters
length_beta <- ncol(mvn_result$var_matrix)
# generate the random samples
beta_samples <- MASS::mvrnorm(n = num_samples,
mu = mvn_result$mode,
Sigma = mvn_result$var_matrix)
# change the data type and name
beta_samples %>%
as.data.frame() %>% tbl_df() %>%
purrr::set_names(sprintf("beta_%02d", (1:length_beta) - 1))
}
post_glm_pred_samples <- function(Xnew, Bmat)
{
# calculate the linear predictor at all prediction points and posterior samples
eta_mat <- as.matrix(Xnew) %*% t(as.matrix(Bmat))
# calculate the event probability
mu_mat <- boot::inv.logit(eta_mat)
# book keeping
list(eta_mat = eta_mat, mu_mat = mu_mat)
}
summarize_glm_pred_from_laplace <- function(mvn_result, Xtest, num_samples)
{
# generate posterior samples of the beta parameters
betas <- generate_glm_post_samples(mvn_result, num_samples)
# data type conversion
betas <- as.matrix(betas)
# make posterior predictions on the test set
pred_test <- post_glm_pred_samples(Xtest, betas)
# calculate summary statistics on the posterior predicted probability
# summarize over the posterior samples
# posterior mean, should you summarize along rows (rowMeans) or
# summarize down columns (colMeans) ???
mu_avg = rowMeans(pred_test$mu_mat)
# posterior quantiles
mu_q05 = apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.05)
mu_q95 = apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.95)
# book keeping
tibble::tibble(
mu_avg = mu_avg,
mu_q05 = mu_q05,
mu_q95 = mu_q95
) %>%
tibble::rowid_to_column("pred_id")
}
post_pred_summary_03_line <- summarize_glm_pred_from_laplace(laplace_03_line, Xmat_03_line, 2500)
post_pred_summary_03_cube <- summarize_glm_pred_from_laplace(laplace_03_cube, Xmat_03_cube, 2500)
dim(post_pred_summary_03_line)
dim(post_pred_summary_03_cube)
post_pred_summary_03_line %>%
mutate(type = "linear relationship") %>%
bind_rows(post_pred_summary_03_cube %>%
mutate(type = "cubic relationship")) %>%
left_join(train_03 %>% tibble::rowid_to_column("pred_id"),
by = "pred_id") %>%
ggplot(mapping = aes(x = x)) +
geom_ribbon(mapping = aes(ymin = mu_q05,
ymax = mu_q95,
group = type),
fill = "steelblue", alpha = 0.5) +
geom_line(mapping = aes(y = mu_avg,
group = type),
color = "navyblue", size = 1.15) +
geom_point(mapping = aes(y = y),
size = 2.5, alpha = 0.25) +
facet_grid( . ~ type) +
labs(y = "y or probability") +
theme_bw()
post_pred_summary_03_line_b <- post_pred_summary_03_line %>% mutate(pred_y = ifelse(mu_avg > 0.5, 1, 0))
post_pred_summary_03_cube_b <- post_pred_summary_03_cube %>% mutate(pred_y = ifelse(mu_avg >0.5, 1, 0))
post_pred_summary_03_line_c <- post_pred_summary_03_line_b %>%
left_join(train_03 %>% tibble::rowid_to_column("pred_id"),
by = "pred_id")
post_pred_summary_03_cube_c <- post_pred_summary_03_cube_b %>%
left_join(train_03 %>% tibble::rowid_to_column("pred_id"),
by = "pred_id")
### confusion matrix for the linear relationship
post_pred_summary_03_line_c %>% count(pred_y, y)
dfaB <- count_(post_pred_summary_03_line_c, vars=c("pred_y", "y"))
dfaB
### confusion matrix for the cubic relationship
### confusion matrix for the linear relationship
post_pred_summary_03_cube_c %>% count(y, pred_y)
exp(laplace_03_line$log_evidence)/exp(laplace_03_cube$log_evidence)
exp(laplace_03_line$log_evidence)/exp(laplace_03_cube$log_evidence)
MAE_01[1] == mae_01_post_01
MAE_01[2] == mae_01_post_02
MAE_01[3] == mae_01_post_03
